"""
äº†è§£ç®—æ³•æ€§èƒ½ä¸­çš„åå·®å’Œæ–¹å·®æ¦‚å¿µ
æ¡ˆä¾‹ï¼šåˆ©ç”¨æ°´åº“æ°´ä½å˜åŒ–é¢„æµ‹å¤§åå‡ºæ°´é‡
æ•°æ®é›†ï¼šex5data1.mat
æœºå™¨å­¦ä¹ è¯¾ç¨‹çš„ç¬¬äº”ä¸ªç¼–ç¨‹ç»ƒä¹ ï¼ˆç¬¬å…­å‘¨è®²è§£å†…å®¹ï¼‰
"""

import numpy as np
import scipy.io as sio
import scipy.optimize as opt
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


def reg_cost(theta, X, y, Î»):
    cost = np.sum(np.power((X @ theta - y.flatten()), 2))
    reg = theta[1:] @ theta[1:] * Î»

    return (cost + reg) / (2 * len(X))


def reg_gradient(theta, X, y, Î»):
    grad = (X @ theta - y.flatten()) @ X
    reg = Î» * theta
    reg[0] = 0

    return (grad + reg) / (len(X))


def linear_regression_model(X, y, Î»):
    theta = np.ones(X.shape[1])

    res = opt.minimize(fun=reg_cost,
                       x0=theta,
                       args=(X, y, Î»),
                       method='TNC',
                       jac=reg_gradient)

    return res.x


def plot_learning_curve(X_train, y_train, X_val, y_val, Î»): # æ„é€ æ ·å“æ•°é‡ä¸è¯¯å·®æ›²çº¿
    x = range(1, len(X_train) + 1)
    training_cost = []
    cv_cost = []
    for i in x:
        res = linear_regression_model(X_train[:i, ], y_train[:i, :], Î»)
        training_cost_i = reg_cost(res, X_train[:i, ], y_train[:i, ], Î»)
        cv_cost_i = reg_cost(res, X_val, y_val, Î»)
        training_cost.append(training_cost_i)
        cv_cost.append(cv_cost_i)
    fig, ax = plt.subplots()
    ax.plot(x, training_cost, label='traning cost')
    ax.plot(x, cv_cost, label='cross_validation cost')
    ax.legend()
    plt.xlabel('number of training examples')
    plt.ylabel('error')


# *****************æ„é€ å¤šé¡¹å¼ç‰¹å¾****************************
def poly_feature(X, power):
    for i in range(2, power + 1):
        X = np.insert(X, X.shape[1], np.power(X[:, 1], i), axis=1)
    return X


def get_means_stds(X):
    means = np.mean(X, axis=0)
    stds = np.std(X, axis=0)
    return means, stds


def feature_normalize(X, means, stds):
    X[:, 1:] = (X[:, 1:] - means[1:]) / stds[1:]

    return X


def plot_data():  # ç»˜åˆ¶æ•£ç‚¹å›¾
    fig, ax = plt.subplots()
    ax.scatter(X_train[:, 1], y_train)
    ax.set(xlabel='change in water level(x)',
           ylabel='water flowing out og the dam(y)')


def plot_poly_fit():
    plot_data()

    x_axis = np.linspace(-60, 60, 100)
    x = x_axis.reshape(100, 1)
    x = np.insert(x, 0, 1, axis=1)
    x = poly_feature(x, power)
    x = feature_normalize(x, train_means, train_stds)
    plt.plot(x_axis, x @ theta_fit, 'r--')


if __name__ == "__main__":
    data = sio.loadmat('ex5data1.mat')
    X_train, y_train = data['X'], data['y']  # (12,1)(12,1)
    X_test, y_test = data['Xtest'], data['ytest']
    print(X_train, y_train.shape)
    df = pd.DataFrame({'water_level': np.reshape(X_train, len(X_train)), 'flow': np.reshape(y_train, len(y_train))})

    # -----------------æ•£ç‚¹å›¾ä¸çº¿æ€§æ‹Ÿåˆçº¿(Figure1)-------------------------
    sns.set(context="notebook", style='darkgrid', palette='deep')
    sns.lmplot('water_level', 'flow', data=df, fit_reg=False, height=5)

    X_train = np.insert(X_train, 0, 1, axis=1)  # X_train-->(12,2)
    theta = np.ones(X_train.shape[1])
    # X_train = np.insert(X, 0, 1, axis=1)

    print(reg_cost(theta, X_train, y_train, Î»=0))
    print(reg_gradient(theta, X_train, y_train, Î»=0))
    final_teta = linear_regression_model(X_train, y_train, Î»=0)
    plt.plot(X_train[:, 1], X_train @ final_teta, c='r')  # æ‹Ÿåˆçº¿

    # --------------------æ ·æœ¬æ•°é‡ä¸è¯¯å·®çš„å˜åŒ–æ›²çº¿(Figure2)----------------
    X_val, y_val = data['Xval'], data['yval']
    X_val = np.insert(X_val, 0, 1, axis=1)
    plot_learning_curve(X_train, y_train, X_val, y_val, Î»=0)

    # -------------æ„é€ å¤šé¡¹å¼ç‰¹å¾ï¼Œè¿›è¡Œå¤šé¡¹å¼å›å½’æ‹Ÿåˆ(Figure3-Figure6)-------------------
    power = 6
    X_test = np.insert(X_test, 0, 1, axis=1)
    X_train_poly = poly_feature(X_train, power)
    X_val_poly = poly_feature(X_val, power)
    X_test_poly = poly_feature(X_test, power)

    train_means, train_stds = get_means_stds(X_train_poly)  # æ¯ä¸€åˆ—(ç‰¹å¾)çš„å‡å€¼å’Œå‡æ–¹æ ¹è¯¯å·®

    X_train_norm = feature_normalize(X_train_poly, train_means, train_stds)
    X_val_norm = feature_normalize(X_val_poly, train_means, train_stds)
    X_test_norm = feature_normalize(X_test_poly, train_means, train_stds)

    theta_fit = linear_regression_model(X_train_norm, y_train, Î»=0)
    plot_poly_fit() # Figure3
    plot_learning_curve(X_train_norm, y_train, X_val_norm, y_val, Î»=0) # Figure4 è¿‡æ‹Ÿåˆ

    theta_fit = linear_regression_model(X_train_norm, y_train, Î»=100)
    plot_poly_fit() # Figure5
    plot_learning_curve(X_train_norm, y_train, X_val_norm, y_val, Î»=100) # Figure6æ­£åˆ™åŒ–ç³»æ•°è¿‡å¤§ï¼Œå˜æˆæ¬ æ‹Ÿåˆäº†

    # -------------------------æ‰¾æœ€ä½³çš„Î» Figure7------------------------------
    Î»_values = [0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]
    training_cost, cv_cost = [], []
    for Î» in Î»_values:
        res_x = linear_regression_model(X_train_norm, y_train, Î»)

        tc = reg_cost(res_x, X_train_norm, y_train, 0)
        cv = reg_cost(res_x, X_val_norm, y_val, 0)

        training_cost.append(tc)
        cv_cost.append(cv)

    fig_1, ax1 = plt.subplots()
    plt.plot(Î»_values, training_cost, label='training cost')
    plt.plot(Î»_values, cv_cost, label='cv cost')
    plt.legend(loc=2)

    plt.xlabel('Î»')
    plt.ylabel('cost')
    Î»_fit = Î»_values[np.argmin(cv_cost)]
    print(Î»_fit) # è°ƒå‚åï¼Œ  ğœ†=0.3  æ˜¯æœ€ä¼˜é€‰æ‹©ï¼Œè¿™ä¸ªæ—¶å€™æµ‹è¯•ä»£ä»·æœ€å°
    # use test data to compute the cost--æµ‹è¯•é›†-
    for Î» in Î»_values:
        theta_ = linear_regression_model(X_train_norm, y_train, Î»)
        print('test cost(Î»={}) = {}'.format(Î», reg_cost(theta_, X_test_norm, y_test, 0)))

    plt.show()
